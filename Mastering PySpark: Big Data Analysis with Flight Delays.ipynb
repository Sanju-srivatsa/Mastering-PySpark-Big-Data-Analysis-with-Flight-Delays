{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3670164,"sourceType":"datasetVersion","datasetId":2196939}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Mastering PySpark: Big Data Analysis with Flight Delays**\n\n### **Introduction to PySpark**\nPySpark is the Python API for Apache Spark, an open-source, distributed computing system designed to process large-scale data. PySpark is widely used for big data processing, machine learning, and streaming due to its scalability, speed, and ease of use.\n\n---\n\n### **Why Use PySpark?**\n1. **Distributed Computing**: Automatically distributes data and computations across a cluster.\n2. **Fault Tolerance**: Ensures data is not lost due to node failures.\n3. **Ease of Use**: Provides simple APIs in Python while harnessing the power of Spark.\n4. **Big Data Ready**: Efficiently handles datasets too large for a single machine.\n5. **Integration**: Compatible with Hadoop, SQL, and Python libraries.\n\n---\n\n### **Core PySpark Concepts and Terminologies**\n\n#### **1. SparkSession**\n- **Definition**: The entry point for any PySpark application, managing configurations and the Spark cluster.\n- **Usage**:\n  ```python\n  from pyspark.sql import SparkSession\n  spark = SparkSession.builder.appName(\"PySpark Guide\").getOrCreate()\n  ```\n\n#### **2. RDD (Resilient Distributed Dataset)**\n- **Definition**: The core data structure in Spark, designed for fault-tolerant and distributed data processing.\n- **Key Features**:\n  - Immutable: Once created, RDDs cannot be modified.\n  - Partitioned: Data is split into partitions for parallel processing.\n  - Lazy Evaluation: Transformations are only executed when an action is triggered.\n- **Operations**:\n  - **Transformations**: `map`, `filter`, `flatMap`, `reduceByKey`.\n  - **Actions**: `collect`, `count`, `take`.\n\n#### **3. DataFrames**\n- **Definition**: A distributed collection of data organized into rows and columns, similar to a database table or Pandas DataFrame.\n- **Features**:\n  - Schema support for structured data.\n  - Optimized for performance with Catalyst Optimizer.\n- **Operations**:\n  - Selecting: `select`, `filter`, `where`.\n  - Aggregations: `groupBy`, `agg`, `count`.\n\n#### **4. Dataset**\n- **Definition**: Strongly-typed DataFrame-like structure introduced for statically-typed languages (Java/Scala). Not common in PySpark.\n\n#### **5. PySpark SQL**\n- **Definition**: Module for querying structured data using SQL.\n- **Key Operations**:\n  - Register a DataFrame as a temporary SQL table:\n    ```python\n    df.createOrReplaceTempView(\"table_name\")\n    spark.sql(\"SELECT * FROM table_name\").show()\n    ```\n\n#### **6. PySpark MLlib**\n- **Definition**: A machine learning library for scalable machine learning.\n- **Key Features**:\n  - Classification and Regression: Logistic regression, linear regression.\n  - Clustering: K-means.\n  - Recommendation Systems: Collaborative filtering.\n  - Feature Engineering: VectorAssembler, StandardScaler.\n  - Model Evaluation: RegressionEvaluator.\n\n#### **7. Transformations**\n- **Definition**: Lazy operations that define how data should be transformed but do not execute immediately.\n- **Examples**:\n  - `map()`: Applies a function to each element.\n  - `filter()`: Filters data based on a condition.\n\n#### **8. Actions**\n- **Definition**: Triggers the execution of transformations and returns a result.\n- **Examples**:\n  - `count()`: Counts the number of elements.\n  - `collect()`: Returns all elements in the RDD/DataFrame.\n\n#### **9. Partitions**\n- **Definition**: Logical divisions of data within an RDD or DataFrame.\n- **Importance**:\n  - Improves parallelism.\n  - Can be controlled using `repartition` or `coalesce`.\n\n#### **10. Catalyst Optimizer**\n- **Definition**: An optimization engine for DataFrame queries to improve execution plans.\n\n#### **11. Tungsten Engine**\n- **Definition**: Optimizes Spark jobs by improving CPU and memory utilization.\n\n#### **12. Lazy Evaluation**\n- **Definition**: Operations are not executed until an action is triggered, optimizing the execution plan.\n\n#### **13. Broadcast Variables**\n- **Definition**: Used to cache a value on all nodes for efficient access.\n- **Usage**:\n  ```python\n  broadcastVar = spark.sparkContext.broadcast([1, 2, 3])\n  ```\n\n#### **14. Accumulators**\n- **Definition**: Write-only shared variables for aggregating data across nodes.\n\n#### **15. PySpark Streaming**\n- **Definition**: Enables real-time processing of streaming data.\n- **Key Concepts**:\n  - DStream: Continuous stream of data.\n  - Window Operations: Aggregations over a sliding window.\n\n#### **16. PySpark Structured Streaming**\n- **Definition**: High-level API for stream processing using SQL and DataFrames.\n\n#### **17. Serialization**\n- **Definition**: Converts data for transmission over the network or storage.\n\n---\n\n### **PySpark Architecture**\n\n1. **Driver Program**:\n   - Manages application lifecycle and scheduling tasks.\n2. **Executors**:\n   - Run tasks assigned by the driver program.\n3. **Cluster Manager**:\n   - Allocates resources for applications (e.g., YARN, Mesos).\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### **PySpark Walkthrough: Learning PySpark Step by Step with Flight Delay Dataset**\n\nThis notebook is designed as a **step-by-step guide** to learn PySpark. It explains each concept in detail using the **Flight Delay dataset** and focuses on building an understanding of PySpark, its operations, and real-world applications.\n","metadata":{}},{"cell_type":"markdown","source":"## **Step 1: Understanding PySpark**\n**What is PySpark?**\n- PySpark is the Python API for Apache Spark.\n- Apache Spark is a distributed computing framework for big data processing.\n- PySpark provides a way to write Spark applications using Python, enabling scalability for large datasets.","metadata":{}},{"cell_type":"markdown","source":"## **Step 2: Install and Import Libraries**\n\n### **Installing PySpark**\nPySpark can be installed using pip:","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:36:07.630461Z","iopub.execute_input":"2024-12-24T04:36:07.630901Z","iopub.status.idle":"2024-12-24T04:36:51.417066Z","shell.execute_reply.started":"2024-12-24T04:36:07.630869Z","shell.execute_reply":"2024-12-24T04:36:51.415627Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Importing Required Libraries**","metadata":{}},{"cell_type":"code","source":"import os  # For working with file paths\nimport time  # To calculate execution times\nimport pandas as pd  # For handling smaller datasets and debugging\nimport pyspark.sql.functions as F  # For transformations and aggregations\nfrom pyspark.sql import SparkSession  # To start a PySpark session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:36:51.419311Z","iopub.execute_input":"2024-12-24T04:36:51.419789Z","iopub.status.idle":"2024-12-24T04:36:51.959186Z","shell.execute_reply.started":"2024-12-24T04:36:51.419727Z","shell.execute_reply":"2024-12-24T04:36:51.957842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 3: Creating a SparkSession**\n\nA `SparkSession` is the starting point for any PySpark application. It allows us to interact with Spark.","metadata":{}},{"cell_type":"code","source":"spark = SparkSession.builder \\\n    .appName(\"Flight Delay Analysis\") \\\n    .getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:36:51.962323Z","iopub.execute_input":"2024-12-24T04:36:51.962999Z","iopub.status.idle":"2024-12-24T04:36:58.119664Z","shell.execute_reply.started":"2024-12-24T04:36:51.962962Z","shell.execute_reply":"2024-12-24T04:36:58.118175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n- `appName`: Assigns a name to the Spark application.\n- `getOrCreate`: Creates a new session or retrieves an existing one.","metadata":{}},{"cell_type":"markdown","source":"## **Step 4: Loading the Dataset**\n\nPySpark allows us to load various types of files such as CSV, JSON, and Parquet. Here, we load the datasets.","metadata":{}},{"cell_type":"code","source":"# File paths for datasets\nfile_path_flights = '/kaggle/input/fligths-delays/flights.csv'\nfile_path_airlines = '/kaggle/input/fligths-delays/airlines.csv'\nfile_path_airports = '/kaggle/input/fligths-delays/airports.csv'\n\n# Loading datasets\nflights = spark.read.csv(file_path_flights, header=True, inferSchema=True)\nairlines = spark.read.csv(file_path_airlines, header=True, inferSchema=True)\nairports = spark.read.csv(file_path_airports, header=True, inferSchema=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:36:58.122796Z","iopub.execute_input":"2024-12-24T04:36:58.123318Z","iopub.status.idle":"2024-12-24T04:37:10.753669Z","shell.execute_reply.started":"2024-12-24T04:36:58.123264Z","shell.execute_reply":"2024-12-24T04:37:10.752324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Options:**\n- `header=True`: Indicates the first row contains column names.\n- `inferSchema=True`: Automatically infers the data types for each column.","metadata":{}},{"cell_type":"markdown","source":"## **Step 5: Exploring the Data**\n\n### **5.1 Displaying Rows**\nUse `.show()` to display rows from a DataFrame:","metadata":{}},{"cell_type":"code","source":"flights.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:10.754964Z","iopub.execute_input":"2024-12-24T04:37:10.755427Z","iopub.status.idle":"2024-12-24T04:37:11.453465Z","shell.execute_reply.started":"2024-12-24T04:37:10.755377Z","shell.execute_reply":"2024-12-24T04:37:11.452299Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.2 Checking Schema**\nUse `.printSchema()` to view the structure of the DataFrame:","metadata":{}},{"cell_type":"code","source":"flights.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:11.454280Z","iopub.execute_input":"2024-12-24T04:37:11.454623Z","iopub.status.idle":"2024-12-24T04:37:11.465381Z","shell.execute_reply.started":"2024-12-24T04:37:11.454593Z","shell.execute_reply":"2024-12-24T04:37:11.463125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\n- Shows the column names, data types, and whether the columns allow null values.","metadata":{}},{"cell_type":"markdown","source":"### **5.3 Counting Rows**","metadata":{}},{"cell_type":"code","source":"print(f\"Number of rows in the Flights dataset: {flights.count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:11.466513Z","iopub.execute_input":"2024-12-24T04:37:11.466976Z","iopub.status.idle":"2024-12-24T04:37:12.844995Z","shell.execute_reply.started":"2024-12-24T04:37:11.466939Z","shell.execute_reply":"2024-12-24T04:37:12.842302Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 6: Data Cleaning**\n\n### **6.1 Handling Missing Data**\nTo check for missing values:","metadata":{}},{"cell_type":"code","source":"# Count missing values for each column\nmissing_values = flights.select([\n    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in flights.columns\n]).collect()[0]\n\n# Format and print the results\nprint(\"Missing Values Per Column:\")\nfor col, count in missing_values.asDict().items():\n    print(f\"{col}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:12.846625Z","iopub.execute_input":"2024-12-24T04:37:12.847397Z","iopub.status.idle":"2024-12-24T04:37:17.262239Z","shell.execute_reply.started":"2024-12-24T04:37:12.847329Z","shell.execute_reply":"2024-12-24T04:37:17.261024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 6.2: Strategies for Handling Missing Data**\n\n#### **1. Dropping Rows or Columns**\n   - Drop rows or columns with missing data if the proportion of missing values is significant or if the column is not critical for analysis.","metadata":{}},{"cell_type":"code","source":"# Drop rows with any missing values\nflights_cleaned = flights.dropna()\n\n# Drop rows with missing values in specific columns\nflights_cleaned = flights.dropna(subset=['TAIL_NUMBER', 'DEPARTURE_TIME'])\n\n# Drop columns with a high proportion of missing values\ncolumns_to_drop = ['CANCELLATION_REASON', 'AIR_SYSTEM_DELAY', 'SECURITY_DELAY', \n                  'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY']\nflights_cleaned = flights.drop(*columns_to_drop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:17.274467Z","iopub.execute_input":"2024-12-24T04:37:17.275042Z","iopub.status.idle":"2024-12-24T04:37:17.368979Z","shell.execute_reply.started":"2024-12-24T04:37:17.274987Z","shell.execute_reply":"2024-12-24T04:37:17.367678Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **2. Filling Missing Values**\n   - Replace missing values with statistical measures or placeholders.","metadata":{}},{"cell_type":"code","source":"# Fill missing numerical values with mean or median\nflights = flights.fillna({'DEPARTURE_DELAY': flights.select(F.mean('DEPARTURE_DELAY')).collect()[0][0]})\n\n# Fill missing categorical values with a placeholder\nflights = flights.fillna({'TAIL_NUMBER': 'Unknown'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:17.371142Z","iopub.execute_input":"2024-12-24T04:37:17.371612Z","iopub.status.idle":"2024-12-24T04:37:19.370109Z","shell.execute_reply.started":"2024-12-24T04:37:17.371564Z","shell.execute_reply":"2024-12-24T04:37:19.368907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.window import Window\n\n# Fill missing values forward\nwindow_spec = Window.partitionBy('ORIGIN_AIRPORT').orderBy('SCHEDULED_DEPARTURE')\nflights = flights.withColumn('DEPARTURE_TIME', \n                            F.last('DEPARTURE_TIME', ignorenulls=True).over(window_spec))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:19.371716Z","iopub.execute_input":"2024-12-24T04:37:19.372749Z","iopub.status.idle":"2024-12-24T04:37:19.475378Z","shell.execute_reply.started":"2024-12-24T04:37:19.372686Z","shell.execute_reply":"2024-12-24T04:37:19.473994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace missing ARRIVAL_DELAY values with DEPARTURE_DELAY + SCHEDULED_TIME (if logical)\nflights = flights.withColumn('ARRIVAL_DELAY', \n                            F.when(F.col('ARRIVAL_DELAY').isNull(), \n                                   F.col('DEPARTURE_DELAY') + F.col('SCHEDULED_TIME')).otherwise(F.col('ARRIVAL_DELAY')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:19.476735Z","iopub.execute_input":"2024-12-24T04:37:19.477246Z","iopub.status.idle":"2024-12-24T04:37:19.561199Z","shell.execute_reply.started":"2024-12-24T04:37:19.477195Z","shell.execute_reply":"2024-12-24T04:37:19.559934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Impute ELAPSED_TIME if AIR_TIME, TAXI_OUT, and TAXI_IN are available\nflights_cleaned = flights_cleaned.withColumn(\n    'ELAPSED_TIME',\n    F.when(F.col('ELAPSED_TIME').isNull() & F.col('AIR_TIME').isNotNull() & F.col('TAXI_OUT').isNotNull() & F.col('TAXI_IN').isNotNull(),\n           F.col('AIR_TIME') + F.col('TAXI_OUT') + F.col('TAXI_IN')).otherwise(F.col('ELAPSED_TIME'))\n)\n\n# Impute AIR_TIME using ELAPSED_TIME\nflights_cleaned = flights_cleaned.withColumn(\n    'AIR_TIME',\n    F.when(F.col('AIR_TIME').isNull() & F.col('ELAPSED_TIME').isNotNull() & F.col('TAXI_OUT').isNotNull() & F.col('TAXI_IN').isNotNull(),\n           F.col('ELAPSED_TIME') - F.col('TAXI_OUT') - F.col('TAXI_IN')).otherwise(F.col('AIR_TIME'))\n)\n\n# Impute WHEELS_ON using WHEELS_OFF and AIR_TIME\nflights_cleaned = flights_cleaned.withColumn(\n    'WHEELS_ON',\n    F.when(F.col('WHEELS_ON').isNull() & F.col('WHEELS_OFF').isNotNull() & F.col('AIR_TIME').isNotNull(),\n           F.col('WHEELS_OFF') + F.col('AIR_TIME')).otherwise(F.col('WHEELS_ON'))\n)\n\n# Impute ARRIVAL_TIME using WHEELS_ON and TAXI_IN\nflights_cleaned = flights_cleaned.withColumn(\n    'ARRIVAL_TIME',\n    F.when(F.col('ARRIVAL_TIME').isNull() & F.col('WHEELS_ON').isNotNull() & F.col('TAXI_IN').isNotNull(),\n           F.col('WHEELS_ON') + F.col('TAXI_IN')).otherwise(F.col('ARRIVAL_TIME'))\n)\n\n# Impute TAXI_IN using ARRIVAL_TIME and WHEELS_ON\nflights_cleaned = flights_cleaned.withColumn(\n    'TAXI_IN',\n    F.when(F.col('TAXI_IN').isNull() & F.col('ARRIVAL_TIME').isNotNull() & F.col('WHEELS_ON').isNotNull(),\n           F.col('ARRIVAL_TIME') - F.col('WHEELS_ON')).otherwise(F.col('TAXI_IN'))\n)\n\n# Impute ARRIVAL_DELAY using ARRIVAL_TIME and SCHEDULED_ARRIVAL\nflights_cleaned = flights_cleaned.withColumn(\n    'ARRIVAL_DELAY',\n    F.when(F.col('ARRIVAL_DELAY').isNull() & F.col('ARRIVAL_TIME').isNotNull() & F.col('SCHEDULED_ARRIVAL').isNotNull(),\n           F.col('ARRIVAL_TIME') - F.col('SCHEDULED_ARRIVAL')).otherwise(F.col('ARRIVAL_DELAY'))\n)\n\n# Recheck missing values after further imputation\nmissing_values_final = flights_cleaned.select([\n    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in flights_cleaned.columns\n]).collect()[0]\n\nprint(\"Missing Values Per Column After Final Handling:\")\nfor col, count in missing_values_final.asDict().items():\n    print(f\"{col}: {count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:19.562255Z","iopub.execute_input":"2024-12-24T04:37:19.562711Z","iopub.status.idle":"2024-12-24T04:37:23.654362Z","shell.execute_reply.started":"2024-12-24T04:37:19.562665Z","shell.execute_reply":"2024-12-24T04:37:23.652737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Step 6.3: Visualizing Missing Data**\n   - Understanding the extent and patterns of missing data is crucial.\n\n   **Visualization**:\n   PySpark doesn’t have built-in visualization tools, so exporting data to Pandas for missing data visualization is an option:","metadata":{}},{"cell_type":"code","source":"# Count missing values for each column after handling\nmissing_values_cleaned = flights_cleaned.select([\n    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in flights_cleaned.columns\n]).collect()[0]\n\n# Format and print the results\nprint(\"Missing Values Per Column After Handling:\")\nfor col, count in missing_values_cleaned.asDict().items():\n    print(f\"{col}: {count}\")\n\n# Convert to Pandas for visualization\nmissing_values_df = pd.DataFrame(flights_cleaned.select([\n    F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in flights_cleaned.columns\n]).collect()[0].asDict().items(), columns=['Column', 'MissingCount'])\n\n# Plot missing values\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nsns.barplot(data=missing_values_df, x='Column', y='MissingCount')\nplt.title(\"Missing Values Per Column After Handling\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Count of Missing Values\")\nplt.xlabel(\"Columns\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:23.656060Z","iopub.execute_input":"2024-12-24T04:37:23.656571Z","iopub.status.idle":"2024-12-24T04:37:31.430139Z","shell.execute_reply.started":"2024-12-24T04:37:23.656524Z","shell.execute_reply":"2024-12-24T04:37:31.428787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.2 Dropping Columns**\nColumns that are not needed for analysis can be dropped:","metadata":{}},{"cell_type":"code","source":"flights = flights.drop('AIR_SYSTEM_DELAY', 'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY', 'WEATHER_DELAY')\n\ncolumns_to_drop = [\n    \"CANCELLATION_REASON\", \n    \"AIR_SYSTEM_DELAY\", \n    \"SECURITY_DELAY\", \n    \"AIRLINE_DELAY\", \n    \"LATE_AIRCRAFT_DELAY\", \n    \"WEATHER_DELAY\"\n]\nflights = flights.drop(*columns_to_drop)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:31.431464Z","iopub.execute_input":"2024-12-24T04:37:31.432202Z","iopub.status.idle":"2024-12-24T04:37:31.465776Z","shell.execute_reply.started":"2024-12-24T04:37:31.432132Z","shell.execute_reply":"2024-12-24T04:37:31.464476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.3 Replacing Null Values**\nReplace null values with default values:","metadata":{}},{"cell_type":"code","source":"flights = flights.fillna({\n    \"DEPARTURE_DELAY\": 0,\n    \"TAXI_OUT\": 0,\n    \"ELAPSED_TIME\": 0,\n    \"AIR_TIME\": 0,\n    \"DISTANCE\": 0\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:31.466863Z","iopub.execute_input":"2024-12-24T04:37:31.467328Z","iopub.status.idle":"2024-12-24T04:37:31.498436Z","shell.execute_reply.started":"2024-12-24T04:37:31.467286Z","shell.execute_reply":"2024-12-24T04:37:31.496871Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 7: Data Transformation**\n\n### **7.1 Adding New Columns**\nCreate a new column for flight duration in hours:","metadata":{}},{"cell_type":"code","source":"flights = flights.withColumn(\"duration_hrs\", F.col(\"AIR_TIME\") / 60)\nflights.select(\"AIR_TIME\", \"duration_hrs\").show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:31.499860Z","iopub.execute_input":"2024-12-24T04:37:31.500356Z","iopub.status.idle":"2024-12-24T04:37:31.784132Z","shell.execute_reply.started":"2024-12-24T04:37:31.500307Z","shell.execute_reply":"2024-12-24T04:37:31.781843Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7.2 Filtering Rows**\nFilter flights with delays greater than 30 minutes:","metadata":{}},{"cell_type":"code","source":"delayed_flights = flights.filter(F.col(\"DEPARTURE_DELAY\") > 30)\ndelayed_flights.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:31.785341Z","iopub.execute_input":"2024-12-24T04:37:31.785941Z","iopub.status.idle":"2024-12-24T04:37:37.835943Z","shell.execute_reply.started":"2024-12-24T04:37:31.785885Z","shell.execute_reply":"2024-12-24T04:37:37.834633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7.3 Grouping and Aggregations**\nFind the average delay for each airline:","metadata":{}},{"cell_type":"code","source":"avg_delay = flights.groupBy(\"AIRLINE\").agg(F.avg(\"DEPARTURE_DELAY\").alias(\"avg_delay\"))\navg_delay.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:37.836941Z","iopub.execute_input":"2024-12-24T04:37:37.837400Z","iopub.status.idle":"2024-12-24T04:37:39.946651Z","shell.execute_reply.started":"2024-12-24T04:37:37.837351Z","shell.execute_reply":"2024-12-24T04:37:39.944171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 8: SQL Queries**\n\nPySpark supports SQL queries using temporary views. \n\n### **8.1 Creating a Temporary View**","metadata":{}},{"cell_type":"code","source":"flights.createOrReplaceTempView(\"flights_view\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:39.947679Z","iopub.execute_input":"2024-12-24T04:37:39.948124Z","iopub.status.idle":"2024-12-24T04:37:40.018775Z","shell.execute_reply.started":"2024-12-24T04:37:39.948078Z","shell.execute_reply":"2024-12-24T04:37:40.017599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **8.2 Running SQL Queries**","metadata":{}},{"cell_type":"code","source":"sql_result = spark.sql(\"\"\"\n    SELECT AIRLINE, AVG(DEPARTURE_DELAY) as avg_delay\n    FROM flights_view\n    GROUP BY AIRLINE\n    ORDER BY avg_delay DESC\n\"\"\")\nsql_result.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:40.019992Z","iopub.execute_input":"2024-12-24T04:37:40.020429Z","iopub.status.idle":"2024-12-24T04:37:42.162131Z","shell.execute_reply.started":"2024-12-24T04:37:40.020387Z","shell.execute_reply":"2024-12-24T04:37:42.159193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 9: Joining DataFrames**\n\n### **Joining Flights with Airlines**","metadata":{}},{"cell_type":"code","source":"flights_with_airlines = flights.join(airlines, flights.AIRLINE == airlines.IATA_CODE, \"inner\")\nflights_with_airlines.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:42.163259Z","iopub.execute_input":"2024-12-24T04:37:42.163711Z","iopub.status.idle":"2024-12-24T04:37:47.042048Z","shell.execute_reply.started":"2024-12-24T04:37:42.163668Z","shell.execute_reply":"2024-12-24T04:37:47.039432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 10: Saving the Data**\n\nSave the DataFrame in Parquet format for optimized storage and querying:","metadata":{}},{"cell_type":"code","source":"flights.write.parquet(\"flights.parquet\", mode=\"overwrite\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:47.043629Z","iopub.execute_input":"2024-12-24T04:37:47.044217Z","iopub.status.idle":"2024-12-24T04:37:58.743114Z","shell.execute_reply.started":"2024-12-24T04:37:47.044135Z","shell.execute_reply":"2024-12-24T04:37:58.741621Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 11: Advanced PySpark**\n\n### **11.1 Caching**\nCache data for faster repeated operations:","metadata":{}},{"cell_type":"code","source":"## **Step 11: Advanced PySpark**\n\nflights.cache()\nprint(flights.is_cached)  # True if cached","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:58.744421Z","iopub.execute_input":"2024-12-24T04:37:58.744868Z","iopub.status.idle":"2024-12-24T04:37:58.824752Z","shell.execute_reply.started":"2024-12-24T04:37:58.744823Z","shell.execute_reply":"2024-12-24T04:37:58.823493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **11.2 Partitioning**\nPartition the data for better performance in distributed systems:","metadata":{}},{"cell_type":"code","source":"flights = flights.repartition(5)  # 5 partitions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:58.825865Z","iopub.execute_input":"2024-12-24T04:37:58.826309Z","iopub.status.idle":"2024-12-24T04:37:58.837432Z","shell.execute_reply.started":"2024-12-24T04:37:58.826263Z","shell.execute_reply":"2024-12-24T04:37:58.836568Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **11.3 Writing Custom Functions**\nApply a custom Python function using `.withColumn`:","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.types import StringType\n\ndef label_delay(delay):\n    return \"High Delay\" if delay > 60 else \"Low Delay\"\n\n# Register the function\nlabel_delay_udf = F.udf(label_delay, StringType())\n\n# Apply to DataFrame\nflights = flights.withColumn(\"Delay_Label\", label_delay_udf(\"DEPARTURE_DELAY\"))\nflights.select(\"DEPARTURE_DELAY\", \"Delay_Label\").show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:37:58.838040Z","iopub.execute_input":"2024-12-24T04:37:58.838309Z","iopub.status.idle":"2024-12-24T04:38:17.325993Z","shell.execute_reply.started":"2024-12-24T04:37:58.838284Z","shell.execute_reply":"2024-12-24T04:38:17.322349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 12: Visualization**\n\nTo visualize the results, convert to Pandas:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Example: Plot average delays per airline\navg_delay_pd = avg_delay.toPandas()\navg_delay_pd.plot(kind='bar', x='AIRLINE', y='avg_delay', legend=False, figsize=(10, 6))\nplt.title(\"Average Delay by Airline\")\nplt.ylabel(\"Average Delay (minutes)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:17.335302Z","iopub.execute_input":"2024-12-24T04:38:17.336574Z","iopub.status.idle":"2024-12-24T04:38:20.306456Z","shell.execute_reply.started":"2024-12-24T04:38:17.336499Z","shell.execute_reply":"2024-12-24T04:38:20.305319Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### **Advanced Topics in PySpark: Machine Learning and Optimization**\n\nThis section explores how to apply **machine learning (ML)** and **optimization techniques** using PySpark. We'll use the **Flight Delay dataset** to predict delays and optimize data processing workflows.\n","metadata":{}},{"cell_type":"markdown","source":"## **Step 13: Introduction to PySpark MLlib**\n\nPySpark’s `MLlib` provides tools for machine learning, including:\n- Data preparation\n- Feature transformation\n- Model training and evaluation\n\n### **MLlib Key Concepts**\n1. **DataFrame-Based API**: Works seamlessly with PySpark DataFrames.\n2. **Pipeline Architecture**: Simplifies building workflows by chaining steps.\n3. **Scalability**: Optimized for big data.","metadata":{}},{"cell_type":"markdown","source":"## **Step 14: Feature Engineering**\n\nMachine learning requires numerical features. Here’s how to prepare the data:\n\n### **14.1 Label Encoding**\nConvert categorical columns (e.g., `AIRLINE`) to numeric values:","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import StringIndexer\n\nindexer = StringIndexer(inputCol=\"AIRLINE\", outputCol=\"AIRLINE_Indexed\")\nflights = indexer.fit(flights).transform(flights)\nflights.select(\"AIRLINE\", \"AIRLINE_Indexed\").show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:20.308753Z","iopub.execute_input":"2024-12-24T04:38:20.309100Z","iopub.status.idle":"2024-12-24T04:38:25.312100Z","shell.execute_reply.started":"2024-12-24T04:38:20.309071Z","shell.execute_reply":"2024-12-24T04:38:25.310868Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **14.2 Vector Assembler**\nCombine multiple columns into a single feature vector:","metadata":{}},{"cell_type":"code","source":"flights.select([F.sum(F.when(F.col(c).isNull(), 1)).alias(c) for c in [\"MONTH_\", \"DAY_\", \"AIRLINE_Indexed\", \"DISTANCE\", \"DEPARTURE_DELAY\"]]).show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:25.313133Z","iopub.execute_input":"2024-12-24T04:38:25.313602Z","iopub.status.idle":"2024-12-24T04:38:27.549007Z","shell.execute_reply.started":"2024-12-24T04:38:25.313556Z","shell.execute_reply":"2024-12-24T04:38:27.547377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"flights = flights.fillna({\n    \"MONTH_\": 1,               # Default to January\n    \"DAY_\": 1,                 # Default to the 1st day of the month\n    \"AIRLINE_Indexed\": 0.0,    # Default to an index value\n    \"DISTANCE\": 0,             # Default distance\n    \"DEPARTURE_DELAY\": 0       # Default delay\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:27.550772Z","iopub.execute_input":"2024-12-24T04:38:27.551265Z","iopub.status.idle":"2024-12-24T04:38:27.592652Z","shell.execute_reply.started":"2024-12-24T04:38:27.551214Z","shell.execute_reply":"2024-12-24T04:38:27.591116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"MONTH_\", \"DAY_\", \"AIRLINE_Indexed\", \"DISTANCE\", \"DEPARTURE_DELAY\"],\n    outputCol=\"features\"\n)\nflights = assembler.transform(flights)\nflights.select(\"features\", \"ARRIVAL_DELAY\").show(5, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:27.593708Z","iopub.execute_input":"2024-12-24T04:38:27.594512Z","iopub.status.idle":"2024-12-24T04:38:29.932909Z","shell.execute_reply.started":"2024-12-24T04:38:27.594472Z","shell.execute_reply":"2024-12-24T04:38:29.930307Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 15: Splitting the Data**\n\nSplit the dataset into training and testing sets:","metadata":{}},{"cell_type":"code","source":"flights.filter(F.col(\"ARRIVAL_DELAY\").isNull()).count()  # Count nulls\nflights.filter(F.isnan(\"ARRIVAL_DELAY\")).count()        # Count NaN values\nflights = flights.fillna({\"ARRIVAL_DELAY\": 0})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:40:42.719788Z","iopub.execute_input":"2024-12-24T04:40:42.720328Z","iopub.status.idle":"2024-12-24T04:40:45.721276Z","shell.execute_reply.started":"2024-12-24T04:40:42.720289Z","shell.execute_reply":"2024-12-24T04:40:45.718324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data, test_data = flights.randomSplit([0.8, 0.2], seed=42)\nprint(f\"Training Data Count: {train_data.count()}\")\nprint(f\"Test Data Count: {test_data.count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:38:29.934189Z","iopub.execute_input":"2024-12-24T04:38:29.934851Z","iopub.status.idle":"2024-12-24T04:38:53.055185Z","shell.execute_reply.started":"2024-12-24T04:38:29.934799Z","shell.execute_reply":"2024-12-24T04:38:53.050663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 16: Building a Machine Learning Model**\n\n### **16.1 Linear Regression**\nTrain a regression model to predict `ARRIVAL_DELAY`:","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(featuresCol=\"features\", labelCol=\"ARRIVAL_DELAY\")\nlr_model = lr.fit(train_data)\n\nprint(f\"Coefficients: {lr_model.coefficients}\")\nprint(f\"Intercept: {lr_model.intercept}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:41:43.912892Z","iopub.execute_input":"2024-12-24T04:41:43.913322Z","iopub.status.idle":"2024-12-24T04:42:03.997739Z","shell.execute_reply.started":"2024-12-24T04:41:43.913286Z","shell.execute_reply":"2024-12-24T04:42:03.996338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **16.2 Evaluating the Model**\nEvaluate the model on test data:","metadata":{}},{"cell_type":"code","source":"test_results = lr_model.evaluate(test_data)\n\nprint(f\"RMSE: {test_results.rootMeanSquaredError}\")\nprint(f\"R^2: {test_results.r2}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:42:23.051802Z","iopub.execute_input":"2024-12-24T04:42:23.052278Z","iopub.status.idle":"2024-12-24T04:42:31.024173Z","shell.execute_reply.started":"2024-12-24T04:42:23.052241Z","shell.execute_reply":"2024-12-24T04:42:31.023135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **16.3 Random Forest for Classification**\nUse Random Forest to classify flights as **on-time** or **delayed**:","metadata":{}},{"cell_type":"code","source":"flights = flights.withColumn(\"Delayed\", F.when(F.col(\"ARRIVAL_DELAY\") > 15, 1).otherwise(0))\nflights.select(\"ARRIVAL_DELAY\", \"Delayed\").show(5)\nflights.printSchema()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:44:06.078899Z","iopub.execute_input":"2024-12-24T04:44:06.079412Z","iopub.status.idle":"2024-12-24T04:44:07.358508Z","shell.execute_reply.started":"2024-12-24T04:44:06.079374Z","shell.execute_reply":"2024-12-24T04:44:07.355394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n\ntrain_data, test_data = flights.randomSplit([0.8, 0.2], seed=42)\n\n# Add a binary label for delay classification\nflights = flights.withColumn(\"Delayed\", F.when(F.col(\"ARRIVAL_DELAY\") > 15, 1).otherwise(0))\n\n# Train the model\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Delayed\")\nrf_model = rf.fit(train_data)\n\n# Predict\npredictions = rf_model.transform(test_data)\npredictions.select(\"Delayed\", \"prediction\", \"probability\").show(5, truncate=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:44:41.369699Z","iopub.execute_input":"2024-12-24T04:44:41.370120Z","iopub.status.idle":"2024-12-24T04:45:36.341861Z","shell.execute_reply.started":"2024-12-24T04:44:41.370087Z","shell.execute_reply":"2024-12-24T04:45:36.340423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **16.4 Model Evaluation**\nEvaluate classification accuracy:","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluator = MulticlassClassificationEvaluator(labelCol=\"Delayed\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(f\"Accuracy: {accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:45:48.648923Z","iopub.execute_input":"2024-12-24T04:45:48.649347Z","iopub.status.idle":"2024-12-24T04:45:58.379795Z","shell.execute_reply.started":"2024-12-24T04:45:48.649311Z","shell.execute_reply":"2024-12-24T04:45:58.378315Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 17: Optimization Techniques**\n\nOptimization is critical for processing large datasets efficiently. Here’s how to optimize PySpark workflows:\n### **17.1 Partitioning**\nRepartitioning improves parallelism for large datasets:","metadata":{}},{"cell_type":"code","source":"flights = flights.repartition(10)  # Repartition to 10 partitions\nprint(f\"Number of partitions: {flights.rdd.getNumPartitions()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:46:11.287835Z","iopub.execute_input":"2024-12-24T04:46:11.288276Z","iopub.status.idle":"2024-12-24T04:46:20.218017Z","shell.execute_reply.started":"2024-12-24T04:46:11.288243Z","shell.execute_reply":"2024-12-24T04:46:20.216760Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **17.2 Caching**\nCache frequently used data:","metadata":{}},{"cell_type":"code","source":"flights.cache()\nflights.count()  # Trigger caching","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:46:20.219446Z","iopub.execute_input":"2024-12-24T04:46:20.219748Z","iopub.status.idle":"2024-12-24T04:46:36.368052Z","shell.execute_reply.started":"2024-12-24T04:46:20.219721Z","shell.execute_reply":"2024-12-24T04:46:36.366895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **17.3 Broadcast Variables**\nUse broadcast variables for small lookup tables:","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import broadcast\n\nairlines_broadcast = broadcast(airlines)\nflights_with_airlines = flights.join(airlines_broadcast, flights.AIRLINE == airlines.IATA_CODE)\nflights_with_airlines.show(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:46:36.370110Z","iopub.execute_input":"2024-12-24T04:46:36.370562Z","iopub.status.idle":"2024-12-24T04:46:36.832626Z","shell.execute_reply.started":"2024-12-24T04:46:36.370515Z","shell.execute_reply":"2024-12-24T04:46:36.831324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **17.4 Using Parquet for Storage**\nParquet is optimized for Spark:","metadata":{}},{"cell_type":"code","source":"flights.write.parquet(\"flights.parquet\", mode=\"overwrite\")\noptimized_flights = spark.read.parquet(\"flights.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:46:36.835355Z","iopub.execute_input":"2024-12-24T04:46:36.836081Z","iopub.status.idle":"2024-12-24T04:46:42.492673Z","shell.execute_reply.started":"2024-12-24T04:46:36.836034Z","shell.execute_reply":"2024-12-24T04:46:42.491230Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Step 18: Visualization**\n\nPySpark lacks native visualization tools, but you can use Pandas and Matplotlib for visualizations:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Convert predictions to Pandas\npredictions_pd = predictions.select(\"Delayed\", \"prediction\").toPandas()\n\n# Plot predictions\npredictions_pd[\"prediction\"].value_counts().plot(kind=\"bar\", title=\"Prediction Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T04:46:42.493936Z","iopub.execute_input":"2024-12-24T04:46:42.494386Z","iopub.status.idle":"2024-12-24T04:46:54.896725Z","shell.execute_reply.started":"2024-12-24T04:46:42.494340Z","shell.execute_reply":"2024-12-24T04:46:54.895423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Conclusion**\n\nPySpark is a robust and versatile framework for big data processing, offering a wide range of tools and functionalities to analyze, transform, and model large datasets efficiently. Through this walkthrough, we explored essential PySpark concepts step-by-step, starting from the basics of creating Spark sessions and loading datasets to advanced techniques like machine learning pipelines, graph processing, and streaming.\n\n### **Key Takeaways**\n1. **Scalability**: PySpark allows you to handle vast amounts of data distributed across clusters, making it ideal for big data analytics.\n2. **Flexibility**: With its modular structure, PySpark supports batch processing, real-time streaming, and machine learning tasks seamlessly.\n3. **Integration**: PySpark integrates well with other tools and libraries like Pandas, SQL, and MLlib, allowing flexibility in workflows.\n4. **Optimization**: Techniques like partitioning, caching, and broadcasting optimize performance for faster execution.\n5. **Real-World Applications**: PySpark is widely used in industries for applications like predictive modeling, fraud detection, and network analysis.\n\n### **Practical Insights**\n- PySpark's DataFrame API simplifies data manipulation, resembling SQL-like operations for ease of use.\n- Pipelines in PySpark streamline the workflow by encapsulating multiple stages of data preprocessing and modeling.\n- Structured Streaming facilitates real-time analytics, making it suitable for monitoring and responding to live events.\n- GraphFrames extend PySpark’s capability for graph-based problems, enabling relationship and connectivity analysis.\n\n### **Future Scope**\nAs data volumes grow exponentially, PySpark will remain a cornerstone of big data analytics. Learning PySpark provides a competitive edge in fields like data science, machine learning, and real-time data engineering. Enhancing skills in advanced topics like Delta Lake, Spark Streaming, and integrating PySpark with cloud services like AWS and Azure will open up more opportunities.\n","metadata":{}}]}